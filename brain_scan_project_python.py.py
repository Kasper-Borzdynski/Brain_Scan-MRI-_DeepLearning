# -*- coding: utf-8 -*-
"""Copy of Opis projektu z przykładowym kodem

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cjCrJH2A_rWf2HDDIJ_O9o_boMq7Qvhg

# Cel projektu
Celem projektu jest przygotowanie metody obcinającej czaszki (skull stripping) na obrazach sekwencji typu T1 rezonansu magnetycznego (MRI) głowy. Metoda powinna z surowego skanu pacjenta wyodrębnić cały obszar zajmowany przez właściwy mózg, pomijając kości, inne tkanki miękkie itp. - ilustracja na rysunku poniżej:
![](https://www.researchgate.net/profile/Dario_Pompili/publication/309402865/figure/fig1/AS:420915604148224@1477365508110/Skull-stripping-steps-A-input-images-B-brain-contouring-and-C-removal-of.png)

Poniżej znajduje się przykładowy kod pobierający, rozpakowujący oraz wczytujący dane do macierzy `numpy` o trzech wymiarach. Dodatkowo załączona jest funkcja wizualizująca środkowe slice'y (przekroje) w każdej z trzech osi macierzy.

# Przykładowy kod z opisami
"""

!pip install --upgrade nibabel

!rm -r FirstDataset
!rm -r SecondDataset
!wget "link hidden, data is not public"
!unzip -q public.zip
!rm public.zip

from google.colab import drive
drive.mount('/content/drive')

!rm -r /content/drive/My Drive/imgzpo/ximage
!rm -r /content/drive/My Drive/imgzpo/xlabel
!mkdir /content/drive/My Drive/imgzpo/ximage
!mkdir /content/drive/My Drive/imgzpo/xlabel

!pip install --upgrade nibabel

import numpy as np
import nibabel as nib
import cv2
from typing import Tuple, List
from pathlib import Path
import os
from google.colab.patches import cv2_imshow
import matplotlib.pyplot as plt

def load_raw_volume(path: Path) -> Tuple[np.ndarray, np.ndarray]:
  data: nib.Nifti1Image = nib.load(str(path))
  data = nib.as_closest_canonical(data)
  raw_data = data.get_fdata(caching='unchanged', dtype=np.float32)
  return raw_data, data.affine


def load_labels_volume(path: Path) -> np.ndarray:
  return load_raw_volume(path)[0].astype(np.uint8)


def save_labels(data: np.ndarray, affine: np.ndarray, path: Path):
  nib.save(nib.Nifti1Image(data, affine), str(path))


def show_slices(slices: List[np.ndarray]):
   fig, axes = plt.subplots(1, len(slices))
   for i, data_slice in enumerate(slices):
       axes[i].imshow(data_slice.T, cmap="gray", origin="lower")

raw_volume, affine = load_raw_volume('/content/FirstDataset/train/e66ce7e96b277d7d1b8835e5d2c94cd6.nii.gz')
mask_volume = load_labels_volume('/content/FirstDataset/train/e66ce7e96b277d7d1b8835e5d2c94cd6_mask.nii.gz')

show_slices([raw_volume[raw_volume.shape[0] // 2], # Środkowy slice 2D w osi x
             raw_volume[:, raw_volume.shape[1] // 2], # Środkowy slice 2D w osi y
             raw_volume[:, :, raw_volume.shape[2] // 2]]) # Środkowy slice 2D w osi z

show_slices([mask_volume[mask_volume.shape[0] // 2], # Środkowy slice 2D w osi x
             mask_volume[:, mask_volume.shape[1] // 2], # Środkowy slice 2D w osi y
             mask_volume[:, :, mask_volume.shape[2] // 2]]) # Środkowy slice 2D w osi z

first_dataset_path = Path('/content/FirstDataset/train')
second_dataset_path = Path('/content/SecondDataset/train')
savepoint = Path('/content/drive/My Drive/imgzpo')
fixcolor = Path('/content/drive/My Drive/imgzpo/x/images/images')

# Zmienna affine zawiera macierz, która będzie potrzebna przy zapisie predykcji do pliku


for dir in os.listdir(second_dataset_path):
  next_path = os.path.join(second_dataset_path,dir)
  for file in os.listdir(next_path):
    if 'mask.nii.gz' in file:
      mask_volume = load_labels_volume(str(next_path)+'/'+str(file))
      for i in range(20,140,20):
        mask = mask_volume[i,:,:]
        plt.imsave(str(savepoint)+'/x/labels/labels/'+str(dir)+str(i)+'.png',mask,cmap="gray", vmin=0, vmax=1)
    else:
      raw_volume, affine = load_raw_volume(str(next_path)+'/'+str(file))
      for i in range(20,140,20):
        raw = raw_volume[i,:,:]
        plt.imsave(str(savepoint)+'/x/images/images/'+str(dir)+str(i)+'.png',raw,cmap="gray")


for i, file in enumerate(os.listdir(first_dataset_path)):
    if file.endswith('mask.nii.gz'):
      mask_volume = load_labels_volume(str(first_dataset_path)+'/'+str(file))
      for i in range(20,140,20):
        mask = mask_volume[i,:,:]
        plt.imsave(str(savepoint)+'/x/labels/labels/'+str(file)+str(i)+'.png',mask, cmap="gray", vmin=0, vmax=1)
    elif file.endswith('nii.gz'):
      raw_volume, affine = load_raw_volume(str(first_dataset_path)+'/'+str(file))
      for i in range(20,140,20):
        raw = raw_volume[i,:,:]
        plt.imsave(str(savepoint)+'/x/images/images/'+str(file)+str(i)+'.png',raw, cmap="gray")

!pip install segmentation_models

import tensorflow as tf
from tensorflow.keras import layers
import segmentation_models as sm
from tensorflow import keras
from PIL import Image


datagen_params = dict(
    validation_split=0.15,

    vertical_flip=True,
    rescale = 1 / 255
)

images_datagen = tf.keras.preprocessing.image.ImageDataGenerator(
    **datagen_params
)

masks_datagen = tf.keras.preprocessing.image.ImageDataGenerator(
    **datagen_params
)

target_size = (256,256)

input_images_generator = images_datagen.flow_from_directory(
    '/content/drive/My Drive/imgzpo/x/images/',
    target_size=target_size,
    batch_size=128,
    class_mode=None,
    seed=42,
    subset='training'
)

input_masks_generator = masks_datagen.flow_from_directory(
    '/content/drive/My Drive/imgzpo/x/labels/',
    target_size=target_size,
    batch_size=128,
    class_mode=None,
    seed=42,
    subset='training'
)

val_images_generator = images_datagen.flow_from_directory(
    '/content/drive/My Drive/imgzpo/x/images/',
    target_size=target_size,
    batch_size=128,
    class_mode=None,
    seed=42,
    subset='validation'
)

val_masks_generator = masks_datagen.flow_from_directory(
    '/content/drive/My Drive/imgzpo/x/labels/',
    target_size=(target_size),
    batch_size=128,
    class_mode=None,
    seed=42,
    subset='validation'
)

train_combined_generator = zip(input_images_generator, input_masks_generator)
validation_combined_generator = zip(val_images_generator, val_masks_generator)

def down_block(x, filters, kernel_size=(3,3), padding="same", strides=1):
  c = keras.layers.Conv2D(filters,kernel_size,padding=padding,strides=strides,activation="relu")(x)
  c = keras.layers.Conv2D(filters,kernel_size,padding=padding,strides=strides,activation="relu")(c)
  p = keras.layers.MaxPool2D((2,2), (2,2))(c)
  return c,p

def up_block(x,skip,filters,kernel_size=(3,3), padding="same", strides=1):
  us = keras.layers.UpSampling2D((2,2))(x)
  concat = keras.layers.Concatenate()([us,skip])
  c = keras.layers.Conv2D(filters,kernel_size,padding=padding,strides=strides,activation="relu")(concat)
  c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides,activation="relu")(c)
  return c 

def bottleneck(x, filters, kernel_size=(3,3), padding="same", strides=1):
  c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides,activation="relu")(x)
  c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides,activation="relu")(c)
  return c

def Unet():
  f = [8,16,32,64,128,256]
  inputs = keras.layers.Input((256,256,3))

  p0 = inputs
  c1,p1 = down_block(p0,f[0]) # 
  c2,p2 = down_block(p1,f[1]) #
  c3,p3 = down_block(p2,f[2]) #
  c4,p4 = down_block(p3,f[3]) #
  c5,p5 = down_block(p4,f[4]) #

  bn = bottleneck(p5, f[5])

  u1 = up_block(bn, c5, f[4]) #
  u2 = up_block(u1, c4, f[3]) #
  u3 = up_block(u2, c3, f[2]) #
  u4 = up_block(u3, c2, f[1]) #
  u5 = up_block(u4,c1,f[0]) #

  outputs = keras.layers.Conv2D(1,(3,3), padding="same", activation="sigmoid")(u5)
  model = keras.models.Model(inputs,outputs)
  return model

model = Unet()
loss = sm.losses.DiceLoss()
model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-3), loss="binary_crossentropy", metrics=["acc"])
model.summary()

training_samples = input_images_generator.n
validation_samples = val_images_generator.n

model.fit(
  train_combined_generator,
  steps_per_epoch=training_samples//128,
  epochs=20,
  validation_data=validation_combined_generator, 
  validation_steps=validation_samples//128
)

model.save_weights("/content/drive/My Drive/imgzpo/finalxmodel/unet.hp")
model.save('/content/drive/My Drive/imgzpo/finalxmodel/')

loaded = tf.keras.models.load_model('/content/drive/My Drive/imgzpo/finalxmodel/')

raw_volume, affine = load_raw_volume('/content/FirstDataset/train/011e40c2b1d3c9bb53bd9e5a7efd04ba.nii.gz')
mask_volume = load_labels_volume('/content/FirstDataset/train/011e40c2b1d3c9bb53bd9e5a7efd04ba_mask.nii.gz')

show_slices([raw_volume[raw_volume.shape[0] // 2], # Środkowy slice 2D w osi x
             raw_volume[:, raw_volume.shape[1] // 2], # Środkowy slice 2D w osi y
             raw_volume[:, :, raw_volume.shape[2] // 2]]) # Środkowy slice 2D w osi z

show_slices([mask_volume[mask_volume.shape[0] // 2], # Środkowy slice 2D w osi x
             mask_volume[:, mask_volume.shape[1] // 2], # Środkowy slice 2D w osi y
             mask_volume[:, :, mask_volume.shape[2] // 2]]) # Środkowy slice 2D w osi z

# raw_volume, affine = load_raw_volume('/content/FirstDataset/test/026719ab1b8e2af45a41ee5b629a12bd.nii.gz')
# plt.imsave('/content/drive/My Drive/imgzpo/test/testx1.png', raw_volume[raw_volume.shape[0] // 2], cmap="gray")

test = plt.imread('/content/drive/My Drive/imgzpo/test/testx1.png')
test = cv2.resize(test,(256,256)).astype(np.float32)
test = cv2.cvtColor(test, cv2.COLOR_BGRA2BGR)
#plt.imshow(test)
print(test.shape[0])

result = loaded.predict(test[None,:])
result = result.squeeze()
print(result.shape)
#plt.imshow(result)

label = np.zeros(result.shape,dtype=np.uint8)
label[result >0.4] = 1
label[label != 1] = 0
#plt.imshow(label)

fig, axes = plt.subplots(1, 2)
axes[0].imshow(label)
axes[1].imshow(test)

# Sprawdźmy rozmiar wokseli

for scan_path in first_dataset_path.iterdir():
  if scan_path.name.endswith('mask.nii.gz'):
    print(nib.load(str(scan_path)).header.get_zooms())


for scan_path in second_dataset_path.iterdir():
  print(nib.load(str(scan_path / 'T1w.nii.gz')).header.get_zooms())

"""Fizyczne rozmiary wokseli są do siebie podobne - raczej nie musimy nic robić z różnicami rzędu 0.2 - 0.3 mm. Alternatywą byłoby wykorzystanie na przykład funkcji `zoom` z biblioteki `scipy` (uwaga - jest wolna). Obecne tutaj różnice mogą nawet paradoksalnie poprawić generalizację sieci.

Na tym etapie należy podzielić dane (najprawdopodobniej ścieżki do nich po wczytaniu ich listy w Pythonie) na zbiór treningowy i walidacyjny, a następnie wyeksportować podzielone dane do dwuwymiarowych obrazów - najlepiej w bezstratnym formacie PNG.

Sugerowany jest podział na zbiór treningowy i walidacyjny już w wyeksportowanych danych na dysku, ale można go oczywiście realizować podczas wczytywania ścieżek danych do uczenia w kodzie źródłowym. Zachęcamy do eksperymentów, ale zaznaczamy, że zazwyczaj lepiej jest przenosić do zbioru walidacyjnego całe skany, a nie losowe przekroje. Przeniesienie losowych przekrojów powoduje, że sieć jest walidowana na fragmencie skanu, którego inne fragmenty są w zbiorze treningowym - mamy gorszą kontrolę przeuczenia, ale z drugiej strony sieć "widzi" podczas treningu więcej różnych skanów.

Przy eksporcie wymagane będzie przeiterowanie się po obu zbiorach danych, wczytywanie kolejnych skanów do pamięci (jak wyżej, ale z wykorzystaniem załączonych funkcji wczytujących), a następnie eksport danych w wybranej osi. Kopię wyeksportowanych danych warto umieścić na podmontowanym Dysku Google. W sytuacji wygaśnięcia sesji Colaba umożliwi to ich późniejsze przekopiowanie na maszynę docelową bez powtarzania procesu eksportu.

Po wyeksportowaniu danych można wykorzystać podejście znane z instrukcji dotyczącej segmentacji. Można testować różne przekroje danych (ostateczne podejście nie musi wcale być jedną siecią - można nauczyć modele dla każdego przekroju), różne modele sieci neuronowych (w tym z pretreningiem i bez), różne loss functions, różne learning rates, różne augmentacje, różne rodzaje normalizacji danych itd.

Po wytrenowaniu ostatecznego modelu można wygenerować predykcje dla zbioru testowego. Poniżej znajduje się ogólnikowy przykład generowania predykcji.
"""

predictions_base_path = Path('/content/drive/My Drive/imgzpo/Predictions')
first_dataset_predictions_path = predictions_base_path / 'first'
second_dataset_predictions_path = predictions_base_path / 'second'

first_dataset_predictions_path.mkdir(exist_ok=True, parents=True)
second_dataset_predictions_path.mkdir(exist_ok=True, parents=True)

first_dataset_test_path = Path('/content/FirstDataset/test')
second_dataset_test_path = Path('/content/SecondDataset/test')

for scan_path in first_dataset_test_path.iterdir():
  data, affine = load_raw_volume(scan_path)
  labels = np.zeros(data.shape, dtype=np.uint8)

  x_slice,y_slice,z_slice = data.shape

  for i in range(data.shape[0]):
    cut_data = data[i,:,:]
    plt.imsave(str(first_dataset_predictions_path)+'/'+'tmp.png',cut_data,cmap="gray")

    _data = plt.imread(str(first_dataset_predictions_path)+'/'+'tmp.png')
    _data = cv2.resize(_data,(256,256)).astype(np.float32)
    _data = cv2.cvtColor(_data, cv2.COLOR_BGRA2BGR)

    result = loaded.predict(_data[None,:])
    result = result.squeeze()

    label = np.zeros(result.shape,dtype=np.uint8)
    label[result >0.5] = 1
    label[label != 1] = 0
    label = cv2.resize(label, (cut_data.shape[1],cut_data.shape[0])).astype(np.uint8)
    labels[i,:,:] = label

  # Tutaj należy przeiterować się na przykład po jednej z osi, wykonać predykcję dla każdego przekroju i wpisać do macierzy labels
  # UWAGA - maska powinna zawierać jedynie wartości 0 i 1
  # UWAGA - predykcje są liczbami zmiennoprzecinkowymi z zakresu 0 do 1 - należy je zbinaryzować wybierając jakiś próg (na przykład 0.5)
  
  save_labels(labels, affine, first_dataset_predictions_path / f'{scan_path.name}')

for scan_path in second_dataset_test_path.iterdir():
  data, affine = load_raw_volume(scan_path / 'T1w.nii.gz')
  labels = np.zeros(data.shape, dtype=np.uint8)

  for i in range(data.shape[0]):
    cut_data = data[i,:,:]
    plt.imsave('/content/drive/My Drive/imgzpo/tmp.png',cut_data,cmap="gray")

    _data = plt.imread('/content/drive/My Drive/imgzpo/tmp.png')
    _data = cv2.resize(_data,(256,256)).astype(np.float32)
    _data = cv2.cvtColor(_data, cv2.COLOR_BGRA2BGR)

    result = loaded.predict(_data[None,:])
    result = result.squeeze()

    label = np.zeros(result.shape,dtype=np.uint8)
    label[result >0.5] = 1
    label[label != 1] = 0
    label = cv2.resize(label, (cut_data.shape[1],cut_data.shape[0])).astype(np.uint8)
    labels[i,:,:] = label

  # Tutaj należy przeiterować się na przykład po jednej z osi, wykonać predykcję dla każdego przekroju i wpisać do macierzy labels
  # UWAGA - maska powinna zawierać jedynie wartości 0 i 1
  # UWAGA - predykcje są liczbami zmiennoprzecinkowymi z zakresu 0 do 1 - należy je zbinaryzować wybierając jakiś próg (na przykład 0.5)
  
  save_labels(labels, affine, second_dataset_predictions_path / f'{scan_path.name}.nii.gz')

import requests
import zlib

predictions_base_path = Path('/content/drive/My Drive/imgzpo/Predictions')
first_dataset_predictions_path = predictions_base_path / 'first'
second_dataset_predictions_path = predictions_base_path / 'second'


for dataset_predictions_path in (first_dataset_predictions_path, second_dataset_predictions_path):
  for prediction_path in dataset_predictions_path.iterdir():
    prediction_name = prediction_path.name[:-7]  # Usuwanie '.nii.gz' z nazwy pliku
    prediction = nib.load(str(prediction_path))

    response = requests.post(f'http://vision.dpieczynski.pl:8080/{prediction_name}', data=zlib.compress(prediction.to_bytes()))
    if response.status_code == 200:
        print(dataset_predictions_path.name, prediction_path.name, response.json())
    else:
        print(f'Error processing prediction {dataset_predictions_path.name}/{prediction_name}: {response.text}')

"""Tak przygotowany katalog `Predictions` należy wgrać do systemu Moodle do oceny."""